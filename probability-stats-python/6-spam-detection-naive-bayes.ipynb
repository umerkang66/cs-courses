{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = [\n",
    "  \"To use your credit, click the new WAP link in the next years txt message or click here\",\n",
    "  \"Thanks for your subscription to New Ringtone UK your new mobile will be charged £5/month Please confirm annoncement by replying\",\n",
    "  \"As a valued customer, I am pleased to advise you that following recent delivery waiting review of your Mob No. you are awarded with. Call us to review.\",\n",
    "  \"Please call our new customer service representative on\",\n",
    "  \"We are trying to contact you. Last weekends customer draw shows that you won a £1000 prize GUARANTEED. Calling years\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave one sentence from spam for testing our model later \n",
    "spam_test = [\"Customer service annoncement. You have a New Years delivery waiting for you. click\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "non = [\n",
    "  \"I don't think he goes to usf, he lives around here though\", \n",
    "  \"New car and house for my parents. i have only new job in hand\", \n",
    "  \"Great escape. I fancy the bridge but needs her lager. See you tomorrow\", \n",
    "  \"Tired. I haven't slept well the past few nights.\",\n",
    "  \"Too late. I said i have the website. I didn't i have or dont have the slippers\", \n",
    "  \"I might come by tonight then if my class lets out early\", \n",
    "  \"Jos ask if u wana meet up?\", \n",
    "  \"That would be great. We'll be at the Guild. We can try meeting with the customer on Bristol road or somewhere\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_test_2 = [\"That would be great. We'll be at the Guild. We can try meeting with the customer on Bristol road or somewhere\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence): \n",
    "  p = PorterStemmer()\n",
    "  removed_stops = remove_stopwords(sentence)\n",
    "  stemmed = p.stem(removed_stops)\n",
    "  tokens = tokenize(stemmed)\n",
    "  return list(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Spams:  [['to', 'use', 'credit', 'click', 'new', 'wap', 'link', 'years', 'txt', 'message', 'click'], ['thanks', 'subscription', 'new', 'ringtone', 'uk', 'new', 'mobile', 'charged', 'month', 'please', 'confirm', 'annoncement', 'repli'], ['as', 'valued', 'customer', 'i', 'pleased', 'advise', 'following', 'recent', 'delivery', 'waiting', 'review', 'mob', 'no', 'awarded', 'with', 'call', 'review'], ['please', 'new', 'customer', 'service', 'repres'], ['we', 'trying', 'contact', 'you', 'last', 'weekends', 'customer', 'draw', 'shows', 'won', 'prize', 'guaranteed', 'calling', 'year']]\n",
      "Tokenized Nons:  [['i', 'don', 't', 'think', 'goes', 'usf', 'l'], ['new', 'car', 'house', 'parents', 'new', 'job', 'hand'], ['great', 'escape', 'i', 'fancy', 'bridge', 'needs', 'lager', 'see', 'tomorrow'], ['tired', 'i', 'haven', 't', 'slept', 'past', 'nights'], ['too', 'late', 'i', 'said', 'website', 'i', 'didn', 't', 'dont', 'slipp'], ['i', 'come', 'tonight', 'class', 'lets', 'earli'], ['jos', 'ask', 'u', 'wana', 'meet', 'up'], ['that', 'great', 'we', 'll', 'guild', 'we', 'try', 'meeting', 'customer', 'bristol', 'road']]\n",
      "Dictionary:  {'pleased', 'valued', 'meet', 'calling', 'txt', 'repli', 'month', 'awarded', 'usf', 'guaranteed', 'repres', 'thanks', 'bristol', 'you', 'prize', 'subscription', 'i', 'following', 'class', 'we', 'message', 'draw', 'haven', 'try', 'new', 'car', 'last', 'didn', 'dont', 'contact', 'job', 'years', 'said', 'guild', 'road', 'click', 'customer', 'trying', 'earli', 'needs', 'link', 'bridge', 'website', 'll', 'to', 'no', 'house', 'that', 'see', 'wana', 'don', 'great', 'service', 'shows', 'tomorrow', 'slept', 'charged', 'advise', 'nights', 'jos', 'ask', 'u', 'use', 'tired', 'won', 'fancy', 'slipp', 'past', 'parents', 'weekends', 'late', 'mobile', 'hand', 'delivery', 'wap', 'review', 'confirm', 'tonight', 't', 'ringtone', 'with', 'l', 'meeting', 'credit', 'please', 'year', 'lager', 'recent', 'call', 'goes', 'come', 'escape', 'lets', 'mob', 'as', 'up', 'waiting', 'think', 'too', 'uk', 'annoncement'}\n"
     ]
    }
   ],
   "source": [
    "dictionary = set() # all unique words\n",
    "spams_tokenized = []\n",
    "non_tokenized = []\n",
    "\n",
    "for sentence in spam: \n",
    "  tokens = tokenize_sentence(sentence)\n",
    "  spams_tokenized.append(tokens)\n",
    "  dictionary = dictionary.union(tokens)\n",
    "\n",
    "for sentence in non: \n",
    "  tokens = tokenize_sentence(sentence)\n",
    "  non_tokenized.append(tokens)\n",
    "  dictionary = dictionary.union(tokens)\n",
    "\n",
    "print(\"Tokenized Spams: \", spams_tokenized)\n",
    "print(\"Tokenized Nons: \", non_tokenized)\n",
    "print(\"Dictionary: \", dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Word Count:  101\n",
      "Total Spam Messages:  5\n",
      "Total All Messages:  13\n"
     ]
    }
   ],
   "source": [
    "total_word_count = len(dictionary)\n",
    "total_spam_messages = len(spams_tokenized)\n",
    "total_all_messages = len(spams_tokenized) + len(non_tokenized)\n",
    "\n",
    "print(\"Total Word Count: \", total_word_count)\n",
    "print(\"Total Spam Messages: \", total_spam_messages)\n",
    "print(\"Total All Messages: \", total_all_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam): 0.38461538461538464\n"
     ]
    }
   ],
   "source": [
    "p_spam = total_spam_messages / total_all_messages\n",
    "print(\"P(spam):\", p_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_in_messages(word, messages): \n",
    "  total_count = 0\n",
    "  for msg in messages: \n",
    "    if word in msg: \n",
    "      total_count+=1\n",
    "  return total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Actual Probability Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', 'great', 'we', 'll', 'guild', 'we', 'try', 'meeting', 'customer', 'bristol', 'road']\n",
      "----------------\n",
      "Running for word: that\n",
      "P( w | spam ) =  0.0\n",
      "P( w ) =  0.07692307692307693\n",
      "P( spam ) =  0.38461538461538464\n",
      "P( spam | w ) =  0.0\n",
      "\n",
      "----------------\n",
      "Running for word: great\n",
      "P( w | spam ) =  0.0\n",
      "P( w ) =  0.15384615384615385\n",
      "P( spam ) =  0.38461538461538464\n",
      "P( spam | w ) =  0.0\n",
      "\n",
      "----------------\n",
      "Running for word: we\n",
      "P( w | spam ) =  0.2\n",
      "P( w ) =  0.15384615384615385\n",
      "P( spam ) =  0.38461538461538464\n",
      "P( spam | w ) =  0.5\n",
      "\n",
      "----------------\n",
      "Running for word: ll\n",
      "P( w | spam ) =  0.0\n",
      "P( w ) =  0.07692307692307693\n",
      "P( spam ) =  0.38461538461538464\n",
      "P( spam | w ) =  0.0\n",
      "\n",
      "----------------\n",
      "Running for word: guild\n",
      "P( w | spam ) =  0.0\n",
      "P( w ) =  0.07692307692307693\n",
      "P( spam ) =  0.38461538461538464\n",
      "P( spam | w ) =  0.0\n",
      "\n",
      "----------------\n",
      "Running for word: we\n",
      "P( w | spam ) =  0.2\n",
      "P( w ) =  0.15384615384615385\n",
      "P( spam ) =  0.38461538461538464\n",
      "P( spam | w ) =  0.5\n",
      "\n",
      "----------------\n",
      "Running for word: try\n",
      "P( w | spam ) =  0.0\n",
      "P( w ) =  0.07692307692307693\n",
      "P( spam ) =  0.38461538461538464\n",
      "P( spam | w ) =  0.0\n",
      "\n",
      "----------------\n",
      "Running for word: meeting\n",
      "P( w | spam ) =  0.0\n",
      "P( w ) =  0.07692307692307693\n",
      "P( spam ) =  0.38461538461538464\n",
      "P( spam | w ) =  0.0\n",
      "\n",
      "----------------\n",
      "Running for word: customer\n",
      "P( w | spam ) =  0.6\n",
      "P( w ) =  0.3076923076923077\n",
      "P( spam ) =  0.38461538461538464\n",
      "P( spam | w ) =  0.75\n",
      "\n",
      "----------------\n",
      "Running for word: bristol\n",
      "P( w | spam ) =  0.0\n",
      "P( w ) =  0.07692307692307693\n",
      "P( spam ) =  0.38461538461538464\n",
      "P( spam | w ) =  0.0\n",
      "\n",
      "----------------\n",
      "Running for word: road\n",
      "P( w | spam ) =  0.0\n",
      "P( w ) =  0.07692307692307693\n",
      "P( spam ) =  0.38461538461538464\n",
      "P( spam | w ) =  0.0\n",
      "\n",
      "P (spam | all_words) =  0.0\n"
     ]
    }
   ],
   "source": [
    "for test_sentence in spam_test_2: \n",
    "  test_sentence = tokenize_sentence(test_sentence)\n",
    "  print(test_sentence)\n",
    "  final_prob = 1\n",
    "\n",
    "  # run this for each word separately\n",
    "  for word in test_sentence: \n",
    "    print(\"----------------\")\n",
    "    print(\"Running for word:\", word)\n",
    "\n",
    "    # P( word/spam )\n",
    "    word_in_spam_count = count_word_in_messages(word, spams_tokenized)\n",
    "    p_w_spam = word_in_spam_count / total_spam_messages\n",
    "    print(\"P( w | spam ) = \", p_w_spam)\n",
    "\n",
    "    # P( word )\n",
    "    word_in_both_datasets = count_word_in_messages(word, spams_tokenized)\n",
    "    word_in_both_datasets += count_word_in_messages(word, non_tokenized)\n",
    "    p_word = word_in_both_datasets / total_all_messages\n",
    "    print(\"P( w ) = \", p_word)\n",
    "\n",
    "    # P( spam / word )\n",
    "    p_spam_w = (p_w_spam * p_spam) / p_word\n",
    "    print(\"P( spam ) = \", p_spam)\n",
    "    print(\"P( spam | w ) = \", p_spam_w)\n",
    "    print(\"\")\n",
    "    final_prob*=p_spam_w\n",
    "\n",
    "  print(\"P (spam | all_words) = \", final_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
